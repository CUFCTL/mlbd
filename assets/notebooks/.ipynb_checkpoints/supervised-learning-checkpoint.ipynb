{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "In the \"Working with Data\" notebook we emphasized the importance of understanding your data. The next step in designing a machine learning system is to __understand your task__. That is, what are you trying to accomplish with youe system? Do you have reason to believe that it can be done by a machine? In theory, if a human can perform some task, a machine can probably perform that task too. This statement comes with many caveats and considerations which we will continue to investigate.\n",
    "\n",
    "In this notebook we will study a class of tasks called __supervised learning__. In supervised learning, the task is to predict an output given an input; in mathematical terms, any supervised learning algorithm simply tries to learn the following:\n",
    "\n",
    "$$y = f(\\vec{x})$$\n",
    "\n",
    "Where $\\vec{x}$ is the input, $y$ is the output, and $f(\\vec{x})$ is the underlying relationship that we are trying to learn. We have already seen an example of such an input / output pair: the Iris dataset. In that dataset, the four features form the input and the species label is the output. We can then imagine a system that predicts the species of a flower based on it's four features, and in fact that's the first thing we'll do in this notebook. This particular task is called __classification__, because the output is a categorical or discrete variable; the other task we'll study is __regression__, in which the output is a continuous value.\n",
    "\n",
    "In this case, we used the entire dataset; in general, however, we may not always do that. A dataset can have more than one type of label, or a lot of features that might not all be relevant. When selecting a dataset to perform a task, we also select which features and label(s) to use based on the task.\n",
    "\n",
    "_Note: some code segments have TODO comments in them. These comments are optional exercises for you to modify the code in a useful way, however they are not meant to be restrictive. Feel free to modify the code in this notebook any way you like; it's a great way to practice your coding skills._\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "You should have your own Anaconda virtual environment with all of the necessary Python modules installed. You can check by trying to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "import sklearn.svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification: Iris Dataset\n",
    "\n",
    "Let's revisit the Iris dataset, but this time with a particular task in mind: to classify an Iris flower based on the four features of the Iris dataset. As a refresher, we'll load and display it in it's raw form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "5             5.4          3.9           1.7          0.4     setosa\n",
       "6             4.6          3.4           1.4          0.3     setosa\n",
       "7             5.0          3.4           1.5          0.2     setosa\n",
       "8             4.4          2.9           1.4          0.2     setosa\n",
       "9             4.9          3.1           1.5          0.1     setosa\n",
       "10            5.4          3.7           1.5          0.2     setosa\n",
       "11            4.8          3.4           1.6          0.2     setosa\n",
       "12            4.8          3.0           1.4          0.1     setosa\n",
       "13            4.3          3.0           1.1          0.1     setosa\n",
       "14            5.8          4.0           1.2          0.2     setosa\n",
       "15            5.7          4.4           1.5          0.4     setosa\n",
       "16            5.4          3.9           1.3          0.4     setosa\n",
       "17            5.1          3.5           1.4          0.3     setosa\n",
       "18            5.7          3.8           1.7          0.3     setosa\n",
       "19            5.1          3.8           1.5          0.3     setosa\n",
       "20            5.4          3.4           1.7          0.2     setosa\n",
       "21            5.1          3.7           1.5          0.4     setosa\n",
       "22            4.6          3.6           1.0          0.2     setosa\n",
       "23            5.1          3.3           1.7          0.5     setosa\n",
       "24            4.8          3.4           1.9          0.2     setosa\n",
       "25            5.0          3.0           1.6          0.2     setosa\n",
       "26            5.0          3.4           1.6          0.4     setosa\n",
       "27            5.2          3.5           1.5          0.2     setosa\n",
       "28            5.2          3.4           1.4          0.2     setosa\n",
       "29            4.7          3.2           1.6          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "120           6.9          3.2           5.7          2.3  virginica\n",
       "121           5.6          2.8           4.9          2.0  virginica\n",
       "122           7.7          2.8           6.7          2.0  virginica\n",
       "123           6.3          2.7           4.9          1.8  virginica\n",
       "124           6.7          3.3           5.7          2.1  virginica\n",
       "125           7.2          3.2           6.0          1.8  virginica\n",
       "126           6.2          2.8           4.8          1.8  virginica\n",
       "127           6.1          3.0           4.9          1.8  virginica\n",
       "128           6.4          2.8           5.6          2.1  virginica\n",
       "129           7.2          3.0           5.8          1.6  virginica\n",
       "130           7.4          2.8           6.1          1.9  virginica\n",
       "131           7.9          3.8           6.4          2.0  virginica\n",
       "132           6.4          2.8           5.6          2.2  virginica\n",
       "133           6.3          2.8           5.1          1.5  virginica\n",
       "134           6.1          2.6           5.6          1.4  virginica\n",
       "135           7.7          3.0           6.1          2.3  virginica\n",
       "136           6.3          3.4           5.6          2.4  virginica\n",
       "137           6.4          3.1           5.5          1.8  virginica\n",
       "138           6.0          3.0           4.8          1.8  virginica\n",
       "139           6.9          3.1           5.4          2.1  virginica\n",
       "140           6.7          3.1           5.6          2.4  virginica\n",
       "141           6.9          3.1           5.1          2.3  virginica\n",
       "142           5.8          2.7           5.1          1.9  virginica\n",
       "143           6.8          3.2           5.9          2.3  virginica\n",
       "144           6.7          3.3           5.7          2.5  virginica\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Iris dataset from seaborn, which provides the dataset as a pandas dataframe\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# display the dataframe\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need something that can perform the classification task... we need a machine learning algorithm! Specifically, we need a __classifier__. Let's go back to our mathematical formulation, but with one added detail:\n",
    "\n",
    "$$y = f(\\vec{x}; \\vec{\\theta})$$\n",
    "\n",
    "Now we say that $f(\\vec{x}; \\vec{\\theta})$ is our __model__, and $\\vec{\\theta}$ is the set of __parameters__ which define our model. This form is one of the most basic ways to describe pretty much any supervised learning algorithm. The important thing to understand is that $\\theta$ can be __literally anything__; we can have as many parameters as we want, and we can arrange them into whatever mathematical expression we want. The learning algorithm (in this case, the classifier) that we choose will determine the structure of these parameters and how they are fitted to best perform the task. We won't explore the mathematical structure of every learning algorithm in these notebooks; for those details we refer you to several well-known resources which can be found on the course website. We only provide this basic formulation to aid your intuition.\n",
    "\n",
    "So the learning algorithm that we choose will give us a model and a way to fit it to our task. There's one more thing: we need a way to __evaluate our model__. To do this, we'll split our dataset into two parts: a __training set__ which will be used to fit the model, and a __testing set__ which will be used to evaluate the model at the end. This procedure is pretty much the same regardless of what learning algorithm we use so let's go ahead and get it out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (105, 4)\n",
      "y_train shape: (105,)\n",
      "X_test shape: (45, 4)\n",
      "y_test shape: (45,)\n"
     ]
    }
   ],
   "source": [
    "# load the Iris dataset from sklearn\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# extract Iris data and labels\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split the Iris dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# print shapes of train set and test set\n",
    "print(\"X_train shape: (%d, %d)\" % X_train.shape)\n",
    "print(\"y_train shape: (%d,)\" % y_train.shape)\n",
    "print(\"X_test shape: (%d, %d)\" % X_test.shape)\n",
    "print(\"y_test shape: (%d,)\" % y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code takes 30% of the dataset at random and uses it for the test set, while using the remaining 70% for the training set. It is common practice to use 20-30% of your data for the test set, so we will leave it as is for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "One of the most basic tasks that you can do with a machine learning algorithm is to ask what category a given data point belongs to. This is what is known as __logistic regression__ in the literature. There are two distinct phases in which this algorithm works. \n",
    "\n",
    "1. __Training Phase__ : In this phase we show our machine learning model data (one at a time) and a corresponding __label__ and continue to do so until we've exhausted our _training set_. For example, if we were looking for cats in a picture we would show \n",
    "our model pictures of different types of cats and tell it that this is a cat. \n",
    "\n",
    "2. __Prediction Phase__: Once we have shown our model enough samples, we now simply ask it to classify an unseen data point into a distinct bin from our _test set_. From our analogy of the cat example, once we've trained our model on cats we can then show our model a picture of say a Bengal \n",
    "cat and if that is classified as a cat we say that our model has trained well. Otherwise we may need to go back to phase 1 all over again with more samples. \n",
    "\n",
    "In this section we're going to build a simple classifier that can predict which species a given flower belongs to a species given some attributes like `sepal width`, `sepal length` etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f320842f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import linear_model \n",
    "\n",
    "# create a logistic regressor\n",
    "logreg = linear_model.LogisticRegression(C=1e5) \n",
    "\n",
    "step_size = .02 \n",
    "# as before we'll use two features for visualization purposes. \n",
    "data = X_train[:, :2]\n",
    "# \"fit\" our data to a logistic regressor. This is same as Phase 1.\n",
    "logreg.fit(data, y_train) \n",
    "\n",
    "# we need to calculate boundaries in x and y dimensions. Since our data is linearly \n",
    "# separable we know there are distinct boundaries within which our data can exist. \n",
    "# In our case, we can simply calculate the min and max of our data (and increase it by some amount so that we can forgive some mistakes of the classifer)\n",
    "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
    "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n",
    "preds = logreg.predict(np.c_[xx.ravel(), yy.ravel()]) # c_ is for concatenating the two arrays, while ravel converts a 2d(or nd) array to 1d array\n",
    "\n",
    "preds = preds.reshape(xx.shape) \n",
    "\n",
    "# let's plot our predictions \n",
    "plt.figure(1, figsize=(10,8)) \n",
    "plt.pcolormesh(xx,yy,preds, cmap=plt.cm.Paired)\n",
    "\n",
    "# and for a good measure our training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1],  edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### k-Nearest Neighbors\n",
    "\n",
    "The first classifier we'll try is called __k-nearest neighbors (kNN)__. Here's how it works: save all of the training samples and their labels, and then when performing classification, label a test sample according to the labels of it's $k$ nearest neighbors in the training set. There are two choices we have to make with this algorithm in order to use it:\n",
    "1. The number of neighbors $k$.\n",
    "2. The distance function, which is used to find the \"nearest neighbors\" of a sample.\n",
    "\n",
    "When $k$ is more than 1, the predicted label is determined by majority vote of the $k$ nearest neighbors. The distance function takes two samples and outputs some measure of \"distance\" between them. We'll start by using $k = 1$ and Euclidean distance:\n",
    "\n",
    "$$d(\\vec{x}, \\vec{y}) = ||\\vec{x} - \\vec{y}||_2$$\n",
    "\n",
    "Now let's see how well a kNN model can perform classification on the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize k-NN model (k = 1)\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(1)\n",
    "\n",
    "# fit the model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict labels for the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# compare the predicted labels to the ground truth labels\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "print(\"%0.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code should output a single number between 0 and 1; this number is the __accuracy__, or the fraction of test samples that were classified correctly. You should get something at or around 0.96, which would mean that the kNN classifier was correct 96% of the time.\n",
    "\n",
    "Is 96% good enough? Once again, that depends entirely on your task, or what you are trying to accomplish. For example, 96% is usually good enough to get the best grade in a class, most people would probably not trust a surgeon who performs well 96% of the time, or an airplane pilot who flies correctly 96% of the time. Furthermore, accuracy is only one example of an __evaluation metric__; there are many other metrics, and although accuracy is one of the easiest to understand intuitively, it is not always the best way to evaluate a model's performance. Later on we will explore some other metrics.\n",
    "\n",
    "Anyway, let's see how kNN performs on Iris as we change the value of $k$. In our usual fashion we'll write a function and experiment with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate kNN\n",
    "def evaluate_knn(k=1):\n",
    "    # initialize k-NN model\n",
    "    knn = sklearn.neighbors.KNeighborsClassifier(k)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # predict labels for the test data\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # compare the predicted labels to the ground truth labels\n",
    "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "    \n",
    "    print(\"k = %d: %0.2f\" % (k, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value $k$ in KNN is an example of what we call a __hyperparameter__. A hyperparameter is like a parameter, except we have to set it ourselves; the model cannot learn a hyperparameter on its own. The distance metric is also a hyperparameter; it is a function that we have to choose. Another very important aspect of designing a machine learning system is to pick the best hyperparameter values, or the values for which the system best performs the task.\n",
    "\n",
    "What we just did is called a __hyperparameter search__ on $k$, and it is the most straightforward way to find the best hyperparameter setting, although it may not always be the most efficient. Keep in mind that we have to train and test the kNN model _every time_ we try a new value of $k$. If we really wanted to be exhaustive with our search, it might take a while. One way to search a large space more efficiently is to search across _orders of magnitude_, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 3, 10, 30, 100]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "    \n",
    "    print(\"k = %3d: %0.2f\" % (k, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: What is the range of possible values for $k$?\n",
    "\n",
    "In the above code, we essentially search through each \"half-order\" of magnitude. This method allows us to quickly sample a large search space, identify the neighborhoods that give the best results, and \"zoom in\" on those neighborhoods to find a more exact value. In this case it looks like the performance only declines as we increase $k$, so maybe we'll just leave $k = 1$ for the Iris classification task.\n",
    "\n",
    "## Classification: Breast Cancer Dataset\n",
    "\n",
    "As it turns out, the Iris dataset is really easy to classify, so let's move on to a more complex dataset. We're going to look at the [UCI ML Breast Cancer Wisconsin dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). You may have already looked at this dataset on your own in the \"Working with Data\" notebook, because it's provided by `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Breast Cancer dataset\n",
    "bc = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "# print dataset stats\n",
    "print(\"X: (%d, %d)\" % bc.data.shape)\n",
    "print(\"y: (%d,)\" % bc.target.shape)\n",
    "print(\"label names: \", bc.target_names)\n",
    "\n",
    "# TODO: create a dataframe with the data, labels, and column names so that you can view everything at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is significantly larger than Iris; in addition to having hundreds of more samples, it has way more features. However, it is simpler in that it has fewer labels: each sample, or tumor, is either malignant or benign. This type of classification is called __binary classification__, and it is the simplest kind of classification we can do.\n",
    "\n",
    "Before we get ahead of ourselves, let's try to visualize the data in some way. It won't be as easy as Iris since we now have 30 features, but one thing we can always do is look at the distributions of invidual features. For that we'll use a `violinplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function for the violinplot\n",
    "def rotate_xticklabels(angle):\n",
    "    for tick in plt.gca().get_xticklabels():\n",
    "        tick.set_horizontalalignment(\"right\")\n",
    "        tick.set_rotation(angle)\n",
    "\n",
    "# create a dataframe for breast cancer dataset\n",
    "df = pd.DataFrame(data=np.c_[bc.data, bc.target], columns=np.append(bc.feature_names, [\"target\"]))\n",
    "\n",
    "# plot distributions of each feature\n",
    "plt.subplots(1, figsize=(20, 5))\n",
    "sns.violinplot(data=df, bw=0.2, cut=1, linewidth=1)\n",
    "rotate_xticklabels(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's still hard to view with 30 features, we can broadly see what each feature looks like. It looks like all of the values are positive, and a few features are spread out _way more_ than the rest.\n",
    "\n",
    "### k-Nearest Neighbors\n",
    "\n",
    "Let's go ahead and try our kNN code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 3, 10, 30, 100]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "    \n",
    "    print(\"k = %3d: %0.2f\" % (k, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results should vary from 90-95%, not quite as good as with Iris. How can we do better? There is one thing: remember how some of the breast cancer features had really high variance? As it turns out, this phenomenon actually throws off most supervised learning algorithms, because it causes them to pay more attention to the high-variance features and less attention to the low-variance features, which might be just as important. Therefore, it is common practice to __scale each feature to have zero mean and unit variance__. That way, the learning algorithm will pay equal attention to each feature.\n",
    "\n",
    "__Question__: In the case of kNN, how would a high-variance feature receive \"more attention\"? _Hint: think about the distance function._\n",
    "\n",
    "Here is where the variance of features comes into play: when features in a dataset don't have the same scale, it actually \"throws off\" most supervised learning algorithms. This scaling is an example of __preprocessing__ our data, or transforming it in some way before feeding it to a machine learning model. With `sklearn` we can scale our data with a single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch breast cancer data and labels\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "# normalize each feature to have zero mean, unit variance\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "\n",
    "# create train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 3, 10, 30, 100]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "\n",
    "    print(\"k = %3d: %0.2f\" % (k, accuracy))\n",
    "\n",
    "# TODO: change evaluate_knn() to use a different distance metric for kNN, see if that improves accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... I guess it's a little better. It's kind of hard to tell right now because the results vary slightly from run to run. Either way, between scaling our data and finding the best hyperparameter settings, there isn't much more that we can do to make kNN perform better on the breast cancer data. We have one more option: try a different learning algorithm.\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "The next algorithm we're going to look at is called a __support vector machine (SVM)__, an especially powerful tool for supervised learning. The good thing is that we won't have to change much to use an SVM. Let's review the steps we've developed up to this point for evaluating a model:\n",
    "\n",
    "1. Pick a dataset\n",
    "2. Scale dataset to zero mean and unit variance\n",
    "3. Split dataset into train set and test set\n",
    "4. Initialize a model with hyperparameter settings\n",
    "5. Fit model to train set\n",
    "6. Evaluate model on test set\n",
    "\n",
    "Notice that the \"model\" can be any classifier, not just kNN, so if we want to use an SVM, we just have to replace kNN with SVM in our code.\n",
    "\n",
    "SVM has two main hyperparameters: the regularization constant $C$, and the kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate an SVM\n",
    "def evaluate_svm(C, kernel):\n",
    "    # initialize SVM model\n",
    "    svm = sklearn.svm.SVC(C=C, kernel=kernel)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # predict labels for the test data\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # compare the predicted labels to the ground truth labels\n",
    "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# evaluate SVM for specific values of C and kernel\n",
    "C = 1.0\n",
    "kernel = \"linear\"\n",
    "\n",
    "accuracy = evaluate_svm(C, kernel)\n",
    "\n",
    "# print results\n",
    "print(\"%0.2f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- talk about SVM vs kNN\n",
    "- experiment with C and linear vs RBF kernel\n",
    "- leave some experimentation as exercise for the students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "TODO:\n",
    "- maybe put before SVM\n",
    "- maybe leave this or SVM as an exercise for the students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression: ??? dataset\n",
    "\n",
    "TODO:\n",
    "- maybe use Boston housing prices from sklearn\n",
    "- linear regression (linear, polynomial)\n",
    "- svm regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "TODO:\n",
    "- briefly mentioned standard scaler and hyperparameter search under Classification\n",
    "- basic cross-validation, train/val/test splitting\n",
    "- visualizations: confusion matrix, ROC curve, correlation scatter plot\n",
    "\n",
    "I want to mention all of these but if it seems like too much then we can put them in a separate section at the end, like this one, so that they have a reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: ???\n",
    "\n",
    "TODO:\n",
    "- probably could extend the previous assignment to simply do a regression / classification task\n",
    "- have them find the best algorithm / hyperparameter combo for their task\n",
    "- could also explore feature selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
