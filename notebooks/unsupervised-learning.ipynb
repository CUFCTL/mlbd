{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In the \"Supervised Learning\" notebook we introduced the idea of a \"task\" and we studied the family of supervised learning tasks, in which the goal is to learn some kind of mapping from labeled data. This description probably begs the question -- what if the data is unlabeled? Would we even be able to do anything with it? It turns out there are a number of things we can still learn about the data; algorithms that work with unlabeled data are called __unsupervised learning__ algorithms.\n",
    "\n",
    "In unsupervised learning, the task is to learn something about the structure of the data. There are a lot of ways we can define the kind of \"structure\" that we want. For example, we could try to group data points into categories based on their values; this task is called __clustering__. As another example, we could take a high-dimensional dataset and try to respresent it somehow in 2D or 3D so that we can visualize it; this task is called __dimensionality reduction__. In this notebook we will look at these two tasks, as they are some of the most common in unsupervised learning. We will continue to use the Iris dataset and other toy datasets from scikit-learn and seaborn.\n",
    "\n",
    "_Note: some code segments have TODO comments in them. These comments are optional exercises for you to modify the code in a useful way, however they are not meant to be restrictive. Feel free to modify the code in this notebook any way you like; it's a great way to practice your coding skills._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You should have your own Anaconda virtual environment with all of the necessary Python modules installed. You can check by trying to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "import sklearn.manifold\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "import sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: Iris Dataset\n",
    "\n",
    "So how would we \"cluster\" the Iris dataset? And why would we want to in the first place? After all, we already have the labels, so we already know what category each sample should belong to. As it turns out, this is a great way to evaluate a clustering algorithm -- if we put aside the labels and cluster the dataset using only the data, we can then compare the clusters identified by the algorithm with the original labels. Remember that one of the first questions we must ask before trying a supervised learning task is whether there is truly a pattern in the data, and clustering algorithms allow us to answer this question.\n",
    "\n",
    "To do this we will take the entire dataset, use a clustering algorithm to identify cluster labels for each sample in the dataset, and then try to compare the labels to the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "One of the most commonly-used clustering algorithms is __k-means clustering__, because it is fast and relatively easy to understand. The algorithm is as follows:\n",
    "\n",
    "1. Select $k$ random samples from the dataset to be the initial means\n",
    "2. Assign a label to each sample in the dataset according to the mean which is nearest to it\n",
    "3. Update each mean to be the centroid of the samples in its cluster\n",
    "4. Repeat steps 2 and 3 until the cluster labels converge\n",
    "\n",
    "Here's an example from Wikipedia of k-means in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif\" alt=\"kmeans\" width=\"300\" height=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm does seem to take the same approach to clustering that k-nearest neighbors takes to classification. In fact, it kind of has the same hyperparameters:\n",
    "1. The number of clusters $k$\n",
    "2. The distance function, which is used to find the nearest mean to a sample\n",
    "\n",
    "As with kNN, we will use Euclidean distance by default. As for the number of clusters $k$, since we already know that the Iris dataset has three classes, we will use $k = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# initialize k-means model\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=3)\n",
    "\n",
    "# fit the model to the dataset\n",
    "kmeans.fit(X)\n",
    "\n",
    "# compute cluster labels for the dataset\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# show a side-by-side comparison of cluster labels and true labels\n",
    "print(np.c_[y, y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the clustering model was able to cluster the dataset into the three classes, but there's one problem: the cluster indices don't necessarily match up to the class indices. In other words, the clustering model doesn't know what each cluster _is_, it only knows how the data points are clustered. What we need is a way to measure the _similarity_ of the cluster labels and the ground truth labels by accounting for permutations. One metric which provides this kind of measure is the __adjusted Rand index (ARI)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari = sklearn.metrics.adjusted_rand_score(y, y_pred)\n",
    "\n",
    "print('%0.3f' % (ari))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARI ranges from -1 to +1, where +1 is perfect similarity. We can also use the scatter plots we developed in the \"Working with Data\" notebook to visually compare the cluster labels with the ground truth labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to plot a slice of Iris dataset\n",
    "def plot_iris_2d(iris, columns, labels):\n",
    "    # extract x and y axes\n",
    "    x, y = iris.data[:, columns[0]], iris.data[:, columns[1]]\n",
    "\n",
    "    # plot x and y\n",
    "    plt.scatter(x, y, c=labels)\n",
    "    plt.xlabel(iris.feature_names[columns[0]])\n",
    "    plt.ylabel(iris.feature_names[columns[1]])\n",
    "\n",
    "# create side-by-side comparison of cluster labels and true labels\n",
    "plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_iris_2d(iris, [0, 2], y_pred)\n",
    "plt.title('Cluster Labels')\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_iris_2d(iris, [0, 2], iris.target)\n",
    "plt.title('Ground Truth Labels')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# TODO: change the value of k and observe its effect on the ARI and the scatter plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see where the clustering model \"mis-clustered\" data points. Note that the colors will match up only if the clustering model happened to assign clusters in the same order as the ground truth labels.\n",
    "\n",
    "It seems like clustering is pretty easy when we have the ground truth labels. But what if we didn't? What if Ronald Fisher had neglected to label each flower that he measured? We would then have just a set of 150 flower measurements with no idea of the species of each flower. Could we use clustering to determine the species? The problem is that in this scenario we don't know the number of species -- in other words, we don't know the value of $k$. We could do a hyperparameter search on $k$, but how would we compare each model? The ARI won't help us here because, again, we don't have the ground truth labels.\n",
    "\n",
    "In the absence of ground truth labels, the primary evaluation metric that we can use for k-means is __inertia__, or within-class scatter. The inertia is the sum of the variance in each cluster; a lower inertia generally corresponds to more coherent clusters, so we will seek a value of $k$ which minimizes the inertia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate k-means for several values of k\n",
    "k_values = range(1, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    model = sklearn.cluster.KMeans(n_clusters=k)\n",
    "    y_pred = model.fit_predict(X)\n",
    "    \n",
    "    print('k = %2d: %0.2f' % (k, model.inertia_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... it seems like the inertia just keeps decreasing as $k$ increases. Well, the largest possible value of $k$ is the number of data points, so let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = iris.data.shape[0]\n",
    "model = sklearn.cluster.KMeans(n_clusters=k)\n",
    "y_pred = model.fit_predict(X)\n",
    "\n",
    "print('k = %2d: %0.2f' % (k, model.inertia_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that there are as many clusters as there are data points, we effectively assign each data point to its own cluster. Since each cluster then has only one data point, the variance of each cluster is 0 and so the total variance, or inertia, is also 0. But this result is not helpful to us. It turns out that the inertia has an inherent __bias__ toward more complex models. The most common way to deal with this bias is to use what's called the __elbow method__, which is best explained with a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kmeans(k):\n",
    "    model = sklearn.cluster.KMeans(n_clusters=k)\n",
    "    model.fit_predict(X)\n",
    "    \n",
    "    return model.inertia_\n",
    "\n",
    "x = range(1, 21)\n",
    "y = [evaluate_kmeans(k) for k in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xticks(x)\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, even though the inertia decreases indefinitely, it also begins to level off, creating an \"elbow\". The elbow method takes the value of $k$ at which this elbow occurs to be the best value of $k$. The idea is that this value gives us the best \"bang for our buck\" -- the simplest model at which the minimum inertia occurs (more or less). But where exactly does the elbow occur? Is it 3? Or 4? Unfortunately, the elbow method isn't an exact method. But can we really be upset? After all, without ground truth labels, the number of clusters in a dataset is quite subjective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [2, 3, 4, 5]\n",
    "\n",
    "plt.subplots(1, len(k_values), figsize=(5 * len(k_values), 5))\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    k = k_values[i]\n",
    "    model = sklearn.cluster.KMeans(n_clusters=k)\n",
    "    labels = model.fit_predict(X)\n",
    "    \n",
    "    plt.subplot(1, len(k_values), i + 1)\n",
    "    plot_iris_2d(iris, [0, 2], labels)\n",
    "    plt.title('k = %d' % (k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters do you see? Can you tell which plot is the most \"correct\"? Clustering becomes much more difficult when we don't have ground truth labels or the number of clusters, because we don't have an objective way to select the best model. This issue pervades virtually every clustering algorithm in existence: __what is the right number of clusters for a dataset__? The philosophical answer is that there is no such thing! Clustering is unsupervised learning; in other words, we don't know what we're looking for. The best we can do is to hand-craft our own metrics based on ideas like inertia, and to use what hints we can get from the task we are trying to do.\n",
    "\n",
    "For a more in-depth overview of k-means and the many other clustering algorithms out there, we refer you to the [scikit-learn documentation](http://scikit-learn.org/stable/modules/clustering.html#clustering). Feel free to try out some of these algorithms yourself on the Iris dataset! A good algorithm to try first is the __Gaussian mixture model__, which is very similar to k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: Digits Dataset\n",
    "\n",
    "We have seen many times by now that datasets tend to have a lot of features, almost always more than 2 or 3, which prevents us from being able to visualize them wholistically. Furthermore, as the number of features in a dataset increases, it becomes harder to extract useful information from the data, to separate \"signal\" from \"noise\", so to speak. We call this phenomenon the __curse of dimensionality__. It is especially true for image datasets, since images can easily contain thousands of pixels. Dimensionality reduction techniques can help us with both of these problems: we can transform a dataset into 2 or 3 dimensions for __visualization__, or more generally we can perform __feature extraction__, in which we transform a dataset into some lower-dimensional space that should contain more signal and less noise.\n",
    "\n",
    "To present these techniques we will use the digits dataset provided by scikit-learn. This dataset consists of 8x8 grayscale images of handwritten digits. We'll go ahead and load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the digits dataset\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "# print dataset stats\n",
    "print('X: (%d, %d)' % digits.data.shape)\n",
    "print('y: (%d,)' % digits.target.shape)\n",
    "print('label names:', digits.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(digits.images[0], square=True, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are very small, and they use only 16 shades of grey. Later on we will look at another dataset of handwritten digits which is much more widely-known, and much larger. But for now this dataset will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a classic dimensionality reduction technique. It has been used for all sorts of things, including visualization, pattern recognition, data compression... and so on. PCA computes the __principal components__ of a dataset. In mathematical terms, the principal components of a dataset $X$ are the eigenvectors of the covariance of $X$:\n",
    "\n",
    "$$W_{pca} = V,$$\n",
    "$$\\Sigma = X X^T = V U V^T$$\n",
    "\n",
    "Intuitively, the principal components of a dataset are the axes along which the variance of the dataset is maximized. They are essentially just features, based on the original features of the dataset. A dataset can have as many principal components as it has features, but we typically only take the $N$ most relevant principal components; this truncation is how we reduce the dimensionality of the data. In particular, we use the (truncated) principal component matrix to project each sample $\\vec{x}$ into a lower-dimensional space:\n",
    "\n",
    "$$\\vec{x}_{proj} = W_{pca} \\vec{x}$$\n",
    "\n",
    "Here's a real-world example. You're in a classroom filled with students, and you want to create a system that can distinguish between each individual student using a set of features. You can use whatever features you want: gender, age, height, weight, skin color, hair color, beard, glasses, clothing, nationality... anything. In this situation, the most __salient__ or useful features would be the ones for which there is a lot of variation between individuals; for example, gender wouldn't be a very useful feature in a room full of guys, but nationality would be very useful in a room full of people from many different nations. In other words, you want the features which exhibit the greatest __variance__ in your dataset of people. PCA attempts to find these features, except that it is not limited to the features themselves; it can also compute new features by using linear combinations of the original features.\n",
    "\n",
    "Now let's try to understand how PCA works with some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data matrix from the digits dataset\n",
    "X = digits.data\n",
    "\n",
    "# create PCA model\n",
    "pca = sklearn.decomposition.PCA()\n",
    "\n",
    "# compute principal components of X\n",
    "pca.fit(X)\n",
    "\n",
    "# print stats\n",
    "print(pca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explained variance of each principal component\n",
    "x = range(pca.n_components_)\n",
    "y = pca.explained_variance_ratio_\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Explained Variance of Principal Components')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we simply compute the principal components of the digits dataset and we plot the explained variance of each component. Components with the highest explained variance are the \"salient features\" that we were talking about; in this case, the first principal component contains nearly 15% of the variance of the entire dataset. Notice that the components are sorted by their explained variance, which makes it easy to take the $n$ components with the most variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the total variance contained within the first n components\n",
    "x = range(pca.n_components_)\n",
    "y = [100 * sum(pca.explained_variance_ratio_[:n]) for n in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Total Explained Variance of Principal Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Total Explained Variance (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we presented the idea of separating the signal from the noise in a dataset; variance is one of the simplest ways to determine where such signal occurs. Much of the useful information in a dataset is in its variations; if we can capture most of a dataset's variance with only a subset of features, we can essentially filter out much of the noise without losing very much signal. In this example, we can take the first 20 components, which reduces the dataset from 64 dimensions to 20 dimensions, and still retain roughly 90% of the information in the dataset. This reduction is useful from two perspectives: on the one hand, we have found a more compact representation of the data (sort of like lossy compression), and on the other hand, we have found features that may be more useful for tasks such as classification.\n",
    "\n",
    "In fact, let's try it -- let's create a classifier and see if using PCA as a preprocessing step improves classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate a kNN classifier (with optional PCA)\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, n_pca=0):\n",
    "    # create the model\n",
    "    model = None\n",
    "\n",
    "    if n_pca != 0:\n",
    "        # if n_pca is specified then create a PCA/kNN model\n",
    "        model = sklearn.pipeline.Pipeline([\n",
    "            ('pca', sklearn.decomposition.PCA(n_components=n_pca)),\n",
    "            ('knn', sklearn.neighbors.KNeighborsClassifier(1))\n",
    "        ])\n",
    "    else:\n",
    "        # otherwise just use kNN\n",
    "        model = sklearn.neighbors.KNeighborsClassifier(1)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the model\n",
    "    return model.score(X_test, y_test)\n",
    "\n",
    "# create train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(digits.data, digits.target, test_size=0.3)\n",
    "\n",
    "# compare the accuracy of several classifiers by varying the number of principal components\n",
    "# a value of 0 means don't use PCA\n",
    "pca_values = [1, 5, 10, 20, 64, 0]\n",
    "\n",
    "for n_pca in pca_values:\n",
    "    accuracy = 100 * evaluate_model(X_train, X_test, y_train, y_test, n_pca)\n",
    "\n",
    "    print('Accuracy (n_pca = %2d): %5.1f%%' % (n_pca, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we didn't really _improve_ the accuracy. The thing is that this dataset is simple enough that kNN can do quite well on its own; to really test the benefit of PCA we would need a more complex dataset, one where kNN doesn't get 98% accuracy out of the box. The more important result here is that _we maintained the same level of accuracy using fewer dimensions_ -- the first 20 principal components provides the same level of classification potential as the original 64 features.\n",
    "\n",
    "We can also use principal components to visualize the dataset. Remember, principal components are like axes, so we can pick any two principal components and project each data point onto them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two principal components by index\n",
    "indices = [0, 1]\n",
    "\n",
    "# project each data point onto the selected axes\n",
    "X_proj = pca.transform(X)\n",
    "\n",
    "x, y = X_proj[:, indices[0]], X_proj[:, indices[1]]\n",
    "\n",
    "# plot the projected data\n",
    "plt.scatter(x, y, c=digits.target, cmap='hsv')\n",
    "plt.xlabel('Principal Component %d' % (indices[0]))\n",
    "plt.ylabel('Principal Component %d' % (indices[1]))\n",
    "plt.show()\n",
    "\n",
    "# TODO: try to find the pair of principal components that best separate the colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went ahead and colored each data points by class. Remember that this dataset consists of images of handwritten digits, so each data point is an image and the class denotes which digit it is. But even with the first two components, the 10 classes are still pretty mixed up. We could separate the classes pretty well if we could use more components -- we know this from the classification experiment that we just ran -- but when we're limited to only two components it's hard to get good class separation. For this reason, PCA is generally pretty limited as a visualization tool, unless the first two components have a very high explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "When it comes to visualizing high-dimensional data, t-SNE is hands-down the most widely-used tool, and arguably the most effective. It is also way more complicated mathematically so we won't even begin to try and explain it. Just know that it can take your high-dimensional dataset and magically put it into 2D or 3D space. Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data matrix from the digits dataset\n",
    "X = digits.data\n",
    "\n",
    "# compute t-SNE of X\n",
    "X_embedded = sklearn.manifold.TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "x, y = X_embedded[:, 0], X_embedded[:, 1]\n",
    "\n",
    "# plot the embedded data\n",
    "plt.axis('off')\n",
    "plt.scatter(x, y, c=digits.target, cmap='hsv')\n",
    "plt.show()\n",
    "\n",
    "# TODO: can you plot t-SNE in 3D?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Look at that class separation.\n",
    "\n",
    "Unlike PCA, t-SNE can't really be used for feature extraction; it just takes a data matrix and transforms it directly into an embedded space. Additionally, if your data has a really large number of features, like thousands or more, you should use another dimensionality reduction method (like PCA) to reduce the data to something on the order of 50 features before using t-SNE, both for better performance and better results.\n",
    "\n",
    "For more information on t-SNE, check out [this article](https://distill.pub/2016/misread-tsne/) which explains how to interpret a t-SNE plot correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_One final thought about dimensionality reduction:_ although it is a particular subtopic within machine learning, in many ways dimensionality reduction is the goal of all machine learning techniques. Whether it is computing summary statistics, fitting a model to a dataset, identifying clusters, or actually reducing a dataset into a few dimensions, all of these techniques seek to transform a (potentially huge) dataset into a few key numbers that describe the dataset with high fidelity. So as you continue to learn about new machine learning techniques, consider how they all serve this one goal in some way or another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: With a Little Help from My Friends\n",
    "\n",
    "One of the main takeaways of unsupervised learning algorithms is that they can be used to improve other learning algorithms, such as the PCA/kNN example. Can you use one of the unsupervised learning algorithms discussed in this notebook to \"help\" another algorithm? Here are some examples to get you started:\n",
    "- use clustering to \"validate\" a classifier on a classification dataset\n",
    "- use PCA to improve the accuracy of a classifier\n",
    "- use PCA to improve t-SNE visualization of a dataset\n",
    "- use t-SNE to visualize a regression dataset (numerical labels vs categorical labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (mlbd)",
   "language": "python",
   "name": "mlbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
