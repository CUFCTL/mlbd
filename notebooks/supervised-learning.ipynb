{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "In the \"Working with Data\" notebook we emphasized the importance of understanding your data. The next step in designing a machine learning system is to __understand your task__. That is, what are you trying to accomplish with youe system? Do you have reason to believe that it can be done by a machine? In theory, if a human can perform some task, a machine can probably perform that task too. This statement comes with many caveats and considerations which we will continue to investigate.\n",
    "\n",
    "In this notebook we will study a class of tasks called __supervised learning__. In supervised learning, the task is to predict an output given an input; in mathematical terms, any supervised learning algorithm simply tries to learn the following:\n",
    "\n",
    "$$y = f(\\vec{x})$$\n",
    "\n",
    "Where $\\vec{x}$ is the input, $y$ is the output, and $f(\\vec{x})$ is the underlying relationship that we are trying to learn. We have already seen an example of such an input / output pair: the Iris dataset. In that dataset, the four features form the input and the species label is the output. We can then imagine a system that predicts the species of a flower based on it's four features, and in fact that's the first thing we'll do in this notebook. This particular task is called __classification__, because the output is a categorical or discrete variable; the other task we'll study is __regression__, in which the output is a continuous value.\n",
    "\n",
    "In this case, we used the entire dataset; in general, however, we may not always do that. A dataset can have more than one type of label, or a lot of features that might not all be relevant. When selecting a dataset to perform a task, we also select which features and label(s) to use based on the task.\n",
    "\n",
    "_Note: some code segments have TODO comments in them. These comments are optional exercises for you to modify the code in a useful way, however they are not meant to be restrictive. Feel free to modify the code in this notebook any way you like; it's a great way to practice your coding skills._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You should have your own Anaconda virtual environment with all of the necessary Python modules installed. You can check by trying to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification: Iris Dataset\n",
    "\n",
    "Let's revisit the Iris dataset, but this time with a particular task in mind: to classify an Iris flower based on the four features of the Iris dataset. As a refresher, we'll load and display it in it's raw form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset from seaborn, which provides the dataset as a pandas dataframe\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# display the dataframe\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need something that can perform the classification task... we need a machine learning algorithm! Specifically, we need a __classifier__. Let's go back to our mathematical formulation, but with one added detail:\n",
    "\n",
    "$$y = f(\\vec{x}; \\vec{\\theta})$$\n",
    "\n",
    "Now we say that $f(\\vec{x}; \\vec{\\theta})$ is our __model__, and $\\vec{\\theta}$ is the set of __parameters__ which define our model. This form is one of the most basic ways to describe pretty much any supervised learning algorithm. The important thing to understand is that $\\theta$ can be __literally anything__; we can have as many parameters as we want, and we can arrange them into whatever mathematical expression we want. The learning algorithm (in this case, the classifier) that we choose will determine the structure of these parameters and how they are fitted to best perform the task. We won't explore the mathematical structure of every learning algorithm in these notebooks; for those details we refer you to several well-known resources which can be found on the course website. We only provide this basic formulation to aid your intuition.\n",
    "\n",
    "So the learning algorithm that we choose will give us a model and a way to fit it to our task. There's one more thing: we need a way to __evaluate our model__. To do this, we'll split our dataset into two parts: a __training set__ which will be used to fit the model, and a __testing set__ which will be used to evaluate the model at the end. This procedure is pretty much the same regardless of what learning algorithm we use so let's go ahead and get it out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset from sklearn\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# extract Iris data and labels\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split the Iris dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# print shapes of train set and test set\n",
    "print('X_train shape: (%d, %d)' % X_train.shape)\n",
    "print('y_train shape: (%d,)' % y_train.shape)\n",
    "print('X_test shape: (%d, %d)' % X_test.shape)\n",
    "print('y_test shape: (%d,)' % y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code takes 30% of the dataset at random and uses it for the test set, while using the remaining 70% for the training set. It is common practice to use 20-30% of your data for the test set, so we will leave it as is for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "One of the most basic classification algorithms is __logistic regression__. It's also a misnomer, as we'll see later, since regression is a different but related task. This algorithm has two distinct phases:\n",
    "\n",
    "1. __Training__: In this phase we repeatedly show our model each sample (with it's label) from the training set, adjusting the model each time to make correct predictions.\n",
    "\n",
    "2. __Prediction__: Once we have shown our model enough samples, we ask it to classify the samples from our test set, __which the model has not seen__. By measuring the percentage of samples that are predicted correctly, we can tell how well the model has learned the task.\n",
    "\n",
    "Let's build a simple classifier using logistic regression to do our classification task. To help your understanding, we'll use just the first two features in Iris so that we can visualize how the classifier \"thinks\" about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logistic regressor\n",
    "clf = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "# \"fit\" the logistic regressor to the training set (phase 1)\n",
    "# we use only the first two features for visualization purposes\n",
    "clf.fit(X_train[:, [0, 1]], y_train) \n",
    "\n",
    "# create a grid of the 2D space in which our training data resides\n",
    "x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "step_size = 0.02\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n",
    "\n",
    "# use the logistic regressor to classify the entire grid\n",
    "# c_ concatenates two arrays, and ravel converts a 2-d (or n-d) array to 1-d array\n",
    "preds = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "preds = preds.reshape(xx.shape) \n",
    "\n",
    "# plot the grid of predictions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(xx, yy, preds, cmap=plt.cm.Paired, shading='auto')\n",
    "\n",
    "# plot the Iris dataset\n",
    "plt.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows you how the logistic regression model approaches the classification task -- it essentially tries to draw straight lines between the three classes in order to separate them. The data points are colored according to their true label so that we can see how well they are separated. It looks like two of the classes are mixed up too much for us to be able to separate them with a straight line. We've seen this before in the Iris dataset. In other words, the data is not __linearly separable__, at least not with these two dimensions.\n",
    "\n",
    "Let's take the above code and put it into a function so that you can repeat this visualization for other pairs of features in Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_space(iris, i, j, clf):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier on Iris using features i and j,\n",
    "    and plot the dataset with the decision boundaries created by the\n",
    "    classifier.\n",
    "    \n",
    "    Args:\n",
    "    iris: Iris dataset\n",
    "    i, j: column indices to use from Iris\n",
    "    clf: classifier to use\n",
    "    \"\"\"\n",
    "    # split the Iris dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "    \n",
    "    # your code here\n",
    "    pass\n",
    "\n",
    "# plot a decision space for several classifiers\n",
    "classifiers = [\n",
    "    ('knn', sklearn.neighbors.KNeighborsClassifier()),\n",
    "    ('lr', sklearn.linear_model.LogisticRegression()),\n",
    "    ('rf', sklearn.ensemble.RandomForestClassifier()),\n",
    "    ('svm', sklearn.svm.SVC())\n",
    "]\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    print(name)\n",
    "    plot_decision_space(iris, 0, 1, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find different feature pairs which can be separated more easily? Even if you can't, remember that we're only using two of the four available features in Iris. The next logical step is to train the classifier with all four features, but we'll leave that for you to try on your own. For now, let's move on to another classifier and take a deeper look at our classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "The next classifier we'll try is called __k-nearest neighbors (kNN)__. Here's how it works: save all of the training samples and their labels, and then when performing classification, label a test sample according to the labels of it's $k$ nearest neighbors in the training set. There are two choices we have to make with this algorithm in order to use it:\n",
    "1. The number of neighbors $k$.\n",
    "2. The distance function, which is used to find the \"nearest neighbors\" of a sample.\n",
    "\n",
    "When $k$ is more than 1, the predicted label is determined by majority vote of the $k$ nearest neighbors. The distance function takes two samples and outputs some measure of \"distance\" between them. We'll start by using $k = 1$ and Euclidean distance:\n",
    "\n",
    "$$d(\\vec{x}, \\vec{y}) = ||\\vec{x} - \\vec{y}||_2$$\n",
    "\n",
    "Now let's see how well a kNN model can perform classification on the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize k-NN model (k = 1)\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(1)\n",
    "\n",
    "# fit the model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict labels for the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# compare the predicted labels to the ground truth labels\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "print('%0.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code should output a single number between 0 and 1; this number is the __accuracy__, or the fraction of test samples that were classified correctly. You should get something at or around 0.96, which would mean that the kNN classifier was correct 96% of the time.\n",
    "\n",
    "Is 96% good enough? Once again, that depends entirely on your task, or what you are trying to accomplish. For example, 96% is usually good enough to get the best grade in a class, most people would probably not trust a surgeon who performs well 96% of the time, or an airplane pilot who flies correctly 96% of the time. Furthermore, accuracy is only one example of an __evaluation metric__; there are many other metrics, and although accuracy is one of the easiest to understand intuitively, it is not always the best way to evaluate a model's performance. Later on we will explore some other metrics.\n",
    "\n",
    "Anyway, let's see how kNN performs on Iris as we change the value of $k$. In our usual fashion we'll write a function and experiment with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate kNN\n",
    "def evaluate_knn(k=1):\n",
    "    # initialize k-NN model\n",
    "    knn = sklearn.neighbors.KNeighborsClassifier(k)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # predict labels for the test data\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # compare the predicted labels to the ground truth labels\n",
    "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "    \n",
    "    print('k = %d: %0.2f' % (k, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value $k$ in KNN is an example of what we call a __hyperparameter__. A hyperparameter is like a parameter, except we have to set it ourselves; the model cannot learn a hyperparameter on its own. The distance metric is also a hyperparameter; it is a function that we have to choose. Another very important aspect of designing a machine learning system is to pick the best hyperparameter values, or the values for which the system best performs the task.\n",
    "\n",
    "What we just did is called a __hyperparameter search__ on $k$, and it is the most straightforward way to find the best hyperparameter setting, although it may not always be the most efficient. Keep in mind that we have to train and test the kNN model _every time_ we try a new value of $k$. If we really wanted to be exhaustive with our search, it might take a while. One way to search a large space more efficiently is to search across _orders of magnitude_, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 3, 10, 30, 100]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "    \n",
    "    print('k = %3d: %0.2f' % (k, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: What is the range of possible values for $k$?\n",
    "\n",
    "In the above code, we essentially search through each \"half-order\" of magnitude. This method allows us to quickly sample a large search space, identify the neighborhoods that give the best results, and \"zoom in\" on those neighborhoods to find a more exact value. Try this with your own results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics: Confusion Matrix\n",
    "\n",
    "Earlier we introduced accuracy as an example of an evaluation metric. While accuracy is easy to understand and compute, it's not always the best metric for classification. In particular, accuracy is not as useful when the dataset is __imbalanced__, that is, when the classes are not evenly sized. As an example, imagine that you had a dataset of patients, 99% of which are healthy and 1% of which have some disease. You could create a classifier which just always outputs \"healthy\" and that classifier would achieve 99% accuracy! With this kind of dataset, it is not enough to consider the overall accuracy; we must also consider how well the model identifies each class. We can visualize these \"class-specific\" accuracies with a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = iris.target_names\n",
    "\n",
    "# create a k-NN model\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict labels for the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# compute confusion matrix for the ground truth and predicted labels\n",
    "cnf_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# plot a heatmap of the confusion matrix\n",
    "sns.heatmap(cnf_matrix, annot=True, fmt='d', cbar=False, square=True, xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Expected')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the classifier achieved 100% accuracy, the confusion matrix would be completely light along the diagonal and completely dark everywhere else. When the classifier makes mistakes, now we can see where it makes them. Lastly, we can condense all of this information into a single value using the __F1 score__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %0.2f' % (sklearn.metrics.accuracy_score(y_test, y_pred)))\n",
    "print('f1 score: %0.2f' % (sklearn.metrics.f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy and F1 scores are very similar for the Iris classifier. Whenever the dataset is balanced (which Iris is), the F1 score tends to be similar to the accuracy. Feel free to try these metrics on the next classification dataset and other toy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: Breast Cancer Dataset\n",
    "\n",
    "As it turns out, the Iris dataset is a really simple dataset, which makes it easy to classify, so let's move on to something more challenging. We're going to look at the [UCI ML Breast Cancer Wisconsin dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). You may have already looked at this dataset on your own in the \"Working with Data\" notebook, because it's provided by scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Breast Cancer dataset\n",
    "bc = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "# print dataset stats\n",
    "print('X: (%d, %d)' % bc.data.shape)\n",
    "print('y: (%d,)' % bc.target.shape)\n",
    "print('label names: ', bc.target_names)\n",
    "\n",
    "# TODO: create a dataframe with the data, labels, and column names so that you can view everything at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is significantly larger than Iris; in addition to having hundreds of more samples, it has way more features. However, it is simpler in that it has fewer labels: each sample, or tumor, is either malignant or benign. This type of classification is called __binary classification__, and it is the simplest kind of classification we can do.\n",
    "\n",
    "Before we get ahead of ourselves, let's try to visualize the data in some way. It won't be as easy as Iris since we now have 30 features, but one thing we can always do is look at the distributions of invidual features. For that we'll use a `violinplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function for the violinplot\n",
    "def rotate_xticklabels(angle):\n",
    "    for tick in plt.gca().get_xticklabels():\n",
    "        tick.set_horizontalalignment('right')\n",
    "        tick.set_rotation(angle)\n",
    "\n",
    "# create a dataframe for breast cancer dataset\n",
    "df = pd.DataFrame(data=np.c_[bc.data, bc.target], columns=np.append(bc.feature_names, ['target']))\n",
    "\n",
    "# plot distributions of each feature\n",
    "plt.figure(figsize=(6, 18))\n",
    "sns.violinplot(data=df, bw=0.2, cut=1, orient='h', linewidth=1)\n",
    "rotate_xticklabels(45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's still hard to view with 30 features, we can broadly see what each feature looks like. It looks like all of the values are positive, and a few features are spread out _way more_ than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "Let's go ahead and try our kNN code from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 3, 10, 30, 100]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "    \n",
    "    print('k = %3d: %0.2f' % (k, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results should vary from 90-95%, not quite as good as with Iris. How can we do better? There is one thing: remember how some of the breast cancer features had really high variance? As it turns out, this phenomenon actually throws off most supervised learning algorithms, because it causes them to pay more attention to the high-variance features and less attention to the low-variance features, which might be just as important. Therefore, it is common practice to __scale each feature to have zero mean and unit variance__. That way, the learning algorithm will pay equal attention to each feature.\n",
    "\n",
    "__Question__: In the case of kNN, how would a high-variance feature receive \"more attention\"? _Hint: think about the distance function._\n",
    "\n",
    "Here is where the variance of features comes into play: when features in a dataset don't have the same scale, it actually \"throws off\" most supervised learning algorithms. This scaling is an example of __preprocessing__ our data, or transforming it in some way before feeding it to a machine learning model. With scikit-learn we can scale our data with a single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch breast cancer data and labels\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "# normalize each feature to have zero mean, unit variance\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "\n",
    "# create train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 3, 10, 30, 100]\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_knn(k)\n",
    "\n",
    "    print('k = %3d: %0.2f' % (k, accuracy))\n",
    "\n",
    "# TODO: change evaluate_knn() to use a different distance metric for kNN, see if that improves accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... I guess it's a little better. It's kind of hard to tell right now because the results vary slightly from run to run. Either way, between scaling our data and finding the best hyperparameter settings, there isn't much more that we can do to make kNN perform better on the breast cancer data. We have one more option: try a different learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "The last classifier we'll try is called a __support vector machine (SVM)__, an especially powerful tool for supervised learning. SVM has two main hyperparameters: the regularization constant $C$, and the kernel function. For more details on how these hyperparameters work, we refer you to the [scikit-learn documentation](http://scikit-learn.org/stable/modules/svm.html), but we'll give you these tips:\n",
    "- The default value of 1 is a good starting point for $C$\n",
    "- The primary options for the kernel are the linear kernel (`'linear'`) and RBF kernel (`'rbf'`). The RBF kernel will probably classify better.\n",
    "\n",
    "The good thing is that we won't have to change much to use an SVM. Let's review the steps we've developed up to this point for evaluating a model:\n",
    "\n",
    "1. Pick a dataset\n",
    "2. Scale dataset to zero mean and unit variance\n",
    "3. Split dataset into train set and test set\n",
    "4. Initialize a model with hyperparameter settings\n",
    "5. Fit model to train set\n",
    "6. Evaluate model on test set\n",
    "\n",
    "Notice that the \"model\" can be any classifier, not just kNN, so if we want to use an SVM, we just have to replace kNN with SVM in our code. We'll leave it to you to experiment with the SVM, using the techniques we've develoepd so far, to see how much accuracy you can achieve on the Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate an SVM\n",
    "def evaluate_svm(C, kernel):\n",
    "    # initialize SVM model\n",
    "    svm = sklearn.svm.SVC(C=C, kernel=kernel)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # predict labels for the test data\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # compare the predicted labels to the ground truth labels\n",
    "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# evaluate SVM for specific values of C and kernel\n",
    "C = 1.0\n",
    "kernel = 'linear'\n",
    "\n",
    "accuracy = evaluate_svm(C, kernel)\n",
    "\n",
    "# print results\n",
    "print('%0.2f' % (accuracy))\n",
    "\n",
    "# TODO: perform hyperparameter search on C and kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics: ROC Curve\n",
    "\n",
    "In the previous section with the Iris dataset we discussed the confusion matrix and F1 score as an alternative to accuracy when the dataset is imbalanced. For binary classification problems like the breast cancer dataset, another useful visualization is the __receiver operating characteristics curve__, or __ROC curve__ for short. The ROC curve is constructed by varying the threshold of the classifier output (for example, logistic regression produces a value between 0 and 1) and measuring the __true positive rate (TPR)__ and __false positive rate (FPR)__ of the resulting predictions. As it happens, TPR and FPR are also related to the confusion matrix, and we leave it as an exercise for you to research this connection.\n",
    "\n",
    "Just as the confusion matrix can be described by the F1 score, the ROC curve can be condensed into a single number by measuring the __area under the curve (AUC)__. Like accuracy and the F1 score, AUC ranges from 0 to 1, so it can be helpful as an additional evaluation metric. As a bonus, the ROC curve can be used to find the optimal threshold for your binary classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train SVM classifier\n",
    "svm = sklearn.svm.SVC(C=1.0, kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# get prediction probabilities for classifier\n",
    "y_score = svm.decision_function(X_test)\n",
    "\n",
    "# compute FPR, TPR, and auc\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_test, y_score)\n",
    "auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr, label='auc = %0.2f' % (auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('Receiver operating characteristics')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# TODO: can you modify the ROC curve to work with more than two classes (like Iris)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the black dashed line represents a baseline of random guessing and the blue curve represents our classifier. A perfect classifier would have an ROC curve that has a TPR of 1 and FPR of 0 in all cases (AUC = 1), but we at least expect our classifier to perform better than random guessing (AUC = 0.5), otherwise what's the point?\n",
    "\n",
    "Notice also that we used `svm.decision_function()` instead of `svm.predict()` to get the classifier output before thresholding. Most scikit-learn classifiers have a method named `decision_function()` or `predict_proba()` for this purpose.\n",
    "\n",
    "Your classifier should have a very high AUC, probably 0.99 or even 1. A high AUC generally means that the classifier output (before thresholding) is very easy to tell apart. We can see this concept more clearly by plotting a histogram of the classifier output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=y_score, hue=y_test)\n",
    "plt.xlabel('Classifier Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression: Boston Housing Prices\n",
    "\n",
    "Now that we've studied classification, the other side of supervised learning will be very easy to cover. The only thing that changes in __regression__ is the nature of the output, namely, that the output is a __real number__ rather than a __category__. A classic example of a regression task is to predict housing prices based on features of a house, and it turns out that scikit-learn has such a dataset among its toy datasets, the Boston house-prices dataset.\n",
    "\n",
    "As always, let's look at the dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Boston house-prices dataset\n",
    "boston = sklearn.datasets.load_boston()\n",
    "\n",
    "# print dataset stats\n",
    "print('X: (%d, %d)' % boston.data.shape)\n",
    "print('y: (%d,)' % boston.target.shape)\n",
    "\n",
    "# create a dataframe for Boston dataset\n",
    "df = pd.DataFrame(\n",
    "    data=np.c_[boston.data, boston.target],\n",
    "    columns=list(boston.feature_names) + ['target']\n",
    ")\n",
    "\n",
    "# show dataframe\n",
    "df\n",
    "\n",
    "# TODO: create a violinplot or other visualizations of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of numbers... what do they all mean? As with the Breast Cancer data, we may not be able to tell immediately what each feature means, for that we would need to consult the dataset source. Fortunately for us, machine learning algorithms don't care about what the features _mean_, they only see the numbers. So we will continue forward, but remember that any specific knowledge about your data can help you make better decisions about your machine learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Probably one of the simplest regression algorithms is __linear regression__. It's so simple, we'll even show you the formula right here. Linear regression assumes that the output value is just a weighted sum of the features:\n",
    "\n",
    "$$y = f(\\vec{x}; \\vec{\\theta}) = f(\\vec{x}; \\vec{w}) = w_1 x_1 + w_2 x_2 + ... + w_n x_n$$\n",
    "\n",
    "Linear regression has a training and prediction phase, just like all of our classifiers, so the code is quite similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch Boston house-prices data and target\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# normalize each feature to have zero mean, unit variance\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "\n",
    "# create train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# initialize linear regression model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict output for the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was easy, but... how do we evaluate the model? The answers are real numbers, not categories, so we can't just count the number of matches. We need a way to measure how _close_ the model gets to the training data. How about this: let's plot the predictions _against_ the ground truth to see how closely they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot expected vs predicted target values\n",
    "vmin = min(min(y_test), min(y_pred))\n",
    "vmax = max(max(y_test), max(y_pred))\n",
    "\n",
    "g = sns.jointplot(x=y_test, y=y_pred)\n",
    "g.ax_joint.plot([vmin, vmax], [vmin, vmax], 'k--')\n",
    "g.ax_joint.set_xlabel('Expected')\n",
    "g.ax_joint.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed line in this plot represents an exact match. While the predictions aren't perfectly on the dashed line, they are clustered around it. These results are what we can generally expect from a good regression model -- predictions that are near the dashed line but have some error. The most common way to quantify how closely the data is to the dashed line is the __correlation__ between the ground truth and the predictions. However, there are a number of other metrics, such as the __explained variance score__ and the __coefficient of determination__ ($r^2$ score), which are considered to be better regression metrics. While the correlation ranges from -1 to 1, the explained variance and $r^2$ scores both range from $-\\infty$ to 1. But in all three cases, higher is better. Here's how to compute all three of them with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = scipy.stats.pearsonr(y_test, y_pred)\n",
    "ev = sklearn.metrics.explained_variance_score(y_test, y_pred)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "print('r   = %0.3f' % (r))\n",
    "print('ev  = %0.3f' % (ev))\n",
    "print('r^2 = %0.3f' % (r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a number of metrics that summarize the actual error between the expected and predicted values, namely __mean squared error__ (MSE) and __mean absolute error__ (MAE). These metrics do exactly what their names suggest. MAE is particularly useful because it quantifies the error in the same units as the target value. For example, the target variable in the Boston dataset is the median house price in thousands of dollars, so we can compare the MAE to the target values to gauge the significance of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = sklearn.metrics.mean_squared_error(y_test, y_pred)\n",
    "mae = sklearn.metrics.mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('median target value = %0.3f' % (np.median(y)))\n",
    "print()\n",
    "print('mse = %0.3f' % (mse))\n",
    "print('mae = %0.3f' % (mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the same way that there are a variety of classification metrics, there are also a bunch of regression metrics out there, and we have to select the most appropriate metric for our situation.\n",
    "\n",
    "In any case, linear regression doesn't have any hyperparameters to tune, so these results are the best we're gonna get unless we try something different..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Polynomial Features\n",
    "\n",
    "In the same way that logistic regression tries to separate the data with straight lines, linear regression essentially tries to fit the data to a straight line. What if the data isn't on a straight line? What if it has curves? It turns out we can also approximate curves with linear regression using __polynomial features__. That is, we compute polynomial terms from our features ($x_1^2, x_1^3, x_1 x_2$, etc.) and use them like regular features in our model.\n",
    "\n",
    "__Question__: Are we still doing linear regression if we use polynomial features?\n",
    "\n",
    "Polynomial features introduce potentially limitless complexity to a linear regression model, which isn't always a good thing. If we keep giving our model more and more degrees of complexity, eventually the model will be able to memorize the training set, learning both the signal and the noise. THis problem is called __overfitting__, and it plagues any machine learning engineer who isn't careful. Fortunately, there are many ways to guard against overfiiting. With linear regression, we can introduce some additional loss terms that will cause the model to minimize the amount of complexity that it uses in addiion to minimizing the prediction error. This technique is known as __L1 and L2 regularization__, and it is provided by the `ElasticNet` model in scikit-learn.\n",
    "\n",
    "Check out the scikit-learn documentation for more detailed explanations of [polynomial features](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions) and [regularization](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net) for linear regression. For now, let's just add the polynomial features and regularization, and see if it does better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear regression model with polynomial features\n",
    "model = sklearn.pipeline.Pipeline([\n",
    "    ('poly', sklearn.preprocessing.PolynomialFeatures(degree=2)),\n",
    "    ('linear', sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=False))\n",
    "])\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict output for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# compute the explained variance score for the model\n",
    "ev = sklearn.metrics.explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print('ev  = %0.3f' % (ev))\n",
    "\n",
    "# TODO: plot expected vs predicted target values for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the polynomial features doesn't change the model, only the input features that are fed to the model. To make things easier we use the `Pipeline` in scikit-learn so that we can just provide the original data and the polynomial features are automatically computed and passed to the model.\n",
    "\n",
    "You can control the number of polynomial features with the `degree` parameter, and you can control the strength of the L1/L2 regularization with the `alpha` and `l1_ratio` parameters. Since we specified `degree=2`, only the second-order features are created. Setting `alpha` and `l1_ratio` in the elastic net model would be the same as using the basic linear regression model. Here's an experiment: increase `degree` to generate even higher-order features, and see what happens. Does it make the model more accurate? Is there a point where more degrees makes the model worse? Can you make higher degrees work well by increasing the L1/L2 penalties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regression\n",
    "\n",
    "Wait, why are we looking at the SVM again? I thought SVM could only do classification? It turns out that SVM can do both! In fact, SVM regression is quite handy in a number of applications, such as predicting bounding boxes for object detection. Since the code is virtually the same as for linear regression, we'll let you try it on your own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize SVM regression model\n",
    "model = sklearn.svm.SVR(kernel='rbf')\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "In the classification and regression examples above, we always created a training set and testing set so that we could evaluate each model on unseen data. Withholding data in this manner is a critical aspect of supervised learning because the entire goal of a supervised learning system is to understand a task given a finite number of examples. If we evaluated a model with data it had already seen, we would quickly run the risk of __overfitting__, in which a model essentially memorizes the training set and fails to learn the actual task.\n",
    "\n",
    "However, there's another bad practice which can lead to overfitting, and we've been doing it throughout this notebook: when we perform hyperparameter search, we've been using the test set to evaluate each model variant, which means that we could potentially overfit _on the test set_. To deal with this problem, we'll have to split our dataset three ways: a training set, a __validation set__, and a testing set. That's a lot of splits, especially for a small dataset, so we use a technique called __cross-validation (CV)__ to make our dataset go a little further. With cross-validation, we will still only create a training set and testing set, but we'll split the training set into __folds__, or equally sized partitions. Then, when we evaluate a model with a particular hyperparameter setting, we'll actually train and evaluate it several times, using a different fold each time as the validation set, and average the scores. This particular method is called __k-fold cross-validation__, and it allows us to compare models without ever touching the test set.\n",
    "\n",
    "Let's just take the first hyperparameter search that we did in this notebook and redo it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# extract data and labels\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# evaluate kNN for several values of k\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "\n",
    "for k in k_values:\n",
    "    # evaluate model using 5-fold cross-validation\n",
    "    knn = sklearn.neighbors.KNeighborsClassifier(k)\n",
    "    scores = sklearn.model_selection.cross_val_score(knn, X_train, y_train, cv=5, n_jobs=-1)\n",
    "    \n",
    "    print('k = %d: %0.2f +/- %0.2f' % (k, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is really easy to use in scikit-learn. Furthermore, it's useful not only for selecting hyperparameters, but also for selecting algorithms! Here's a simple example where we compare all of the classifiers discussed in this notebook on the Breast Cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Breast Cancer dataset\n",
    "bc = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "# extract data and labels\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# evaluate several classifiers on the training set\n",
    "models = [\n",
    "    ('lr', sklearn.linear_model.LogisticRegression()),\n",
    "    ('knn', sklearn.neighbors.KNeighborsClassifier(1)),\n",
    "    ('rf', sklearn.ensemble.RandomForestClassifier()),\n",
    "    ('svm', sklearn.svm.SVC(kernel='linear'))\n",
    "]\n",
    "\n",
    "for name, clf in models:\n",
    "    # evaluate model using 5-fold cross-validation\n",
    "    scores = sklearn.model_selection.cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1)\n",
    "    \n",
    "    print('%8s: %0.2f +/- %0.2f' % (name, scores.mean(), scores.std()))\n",
    "\n",
    "# TODO: expand this code to compare hyperparameters and algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: How High Can You Go\n",
    "\n",
    "This assignment is essentially an extension of what you did in the previous notebook: pick a toy dataset from [scikit-learn](http://scikit-learn.org/stable/datasets/index.html#toy-datasets) or [seaborn](http://seaborn.pydata.org/generated/seaborn.load_dataset.html#seaborn.load_dataset), even the same one you used from before, and train a classifier or regressor on the dataset. Your goal is to get the highest accuracy possible, using any of the algorithms and techniques we've developed in this notebook. Keep in mind some of the questions you should be asking yourself:\n",
    "- Are the labels __categorical__ or __numerical__ (classification or regression)?\n",
    "- Which evaluation metric(s) are more appropriate for your dataset? \n",
    "- Which algorithm performs the best?\n",
    "- Which hyperparameter settings work best for each algorithm?\n",
    "\n",
    "Note that all of the previously-developed techniques for understanding your data still apply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (mlbd)",
   "language": "python",
   "name": "mlbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
